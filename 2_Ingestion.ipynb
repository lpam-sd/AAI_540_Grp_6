{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "181771eb-cdbf-44cf-918c-da89de113e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-studio 1.1.4 requires pydynamodb>=0.7.4, which is not installed.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/opt/conda/bin/python', '-m', 'pip', 'install', '-q', 'kaggle'], returncode=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install kaggle CLI into the current notebook environment\n",
    "import sys, subprocess\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"kaggle\"], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "508eb9fe-1789-46f5-8fca-60c6608eb3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/kaggle\n"
     ]
    }
   ],
   "source": [
    "# Find the kaggle executable on PATH\n",
    "import shutil\n",
    "print(shutil.which(\"kaggle\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "413b4f55-b6fb-42ef-85b9-57bd245deda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/crawford/cat-dataset\n",
      "License(s): CC0-1.0\n",
      "Downloading cat-dataset.zip to data/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.04G/4.04G [00:28<00:00, 151MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Sample files: [PosixPath('data/raw/cats'), PosixPath('data/raw/CAT_00'), PosixPath('data/raw/CAT_01'), PosixPath('data/raw/CAT_02'), PosixPath('data/raw/CAT_03'), PosixPath('data/raw/CAT_04'), PosixPath('data/raw/CAT_05'), PosixPath('data/raw/CAT_06'), PosixPath('data/raw/cifar10')]\n"
     ]
    }
   ],
   "source": [
    "# Download + unzip dataset using kaggle CLI\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "raw_dir = Path(\"data/raw\")\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "subprocess.run([\n",
    "    \"kaggle\", \"datasets\", \"download\",\n",
    "    \"-d\", \"crawford/cat-dataset\",\n",
    "    \"-p\", str(raw_dir),\n",
    "    \"--unzip\"\n",
    "], check=True)\n",
    "\n",
    "print(\"Done. Sample files:\", list(raw_dir.iterdir())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae32746f-c4f8-4637-863e-9509bd5c3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Parse per-image landmark files  into a single annotations CSV\n",
    "#    Each file typically contains 9 landmark points (x,y) in a fixed order.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "processed_dir = Path(\"data/processed/cats\")\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cats_images_dir = raw_dir  # adjust if unzip created subfolders\n",
    "rows = []\n",
    "\n",
    "for img_path in cats_images_dir.rglob(\"*.jpg\"):\n",
    "    # Example: annotation file might be \"<image>.cat\" or similar naming\n",
    "    ann_path = img_path.with_suffix(img_path.suffix + \".cat\")  # adjust if your files differ\n",
    "    if not ann_path.exists():\n",
    "        continue\n",
    "\n",
    "    # Read and parse annotation file\n",
    "    text = ann_path.read_text().strip().split()\n",
    "    # Common format: first number = count, then pairs x y...\n",
    "    # (You may need to adapt depending on exact file content)\n",
    "    pts = list(map(float, text[1:]))  # skip count\n",
    "    rows.append({\"image\": str(img_path), \"points\": pts})\n",
    "\n",
    "cats_ann = pd.DataFrame(rows)\n",
    "# Expand pts into columns x1,y1,...,x9,y9\n",
    "pts_cols = [f\"{axis}{i}\" for i in range(1, 10) for axis in (\"x\", \"y\")]\n",
    "cats_ann[pts_cols] = pd.DataFrame(cats_ann[\"points\"].tolist(), index=cats_ann.index)\n",
    "cats_ann = cats_ann.drop(columns=[\"points\"])\n",
    "\n",
    "cats_ann.to_csv(\"data/processed/cats/annotations.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b69e38-6c0f-4776-a23b-6a7f7609c74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the file exists locally\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"data/processed/cats/annotations.csv\").exists()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da10e4dd-2418-43f5-9114-3a1d3dacf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the locally generated annotations file to S3\n",
    "\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "bucket = \"sagemaker-us-east-1-549206572067\"\n",
    "s3_key = \"cat-landmarks-project/raw/cats/annotations.csv\"\n",
    "\n",
    "s3.upload_file(\n",
    "    \"data/processed/cats/annotations.csv\",\n",
    "    bucket,\n",
    "    s3_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e78a432-ca3a-46af-8524-e96302fc60aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_251/990443597.py:15: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path=cifar_tar.parent)\n"
     ]
    }
   ],
   "source": [
    "# 3) Download and extract CIFAR-10 python dataset\n",
    "#    This produces cifar-10-batches-py with pickled batches.\n",
    "\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "cifar_url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"  # official\n",
    "cifar_tar = Path(\"data/raw/cifar10/cifar-10-python.tar.gz\")\n",
    "cifar_tar.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve(cifar_url, cifar_tar)\n",
    "\n",
    "with tarfile.open(cifar_tar, \"r:gz\") as tar:\n",
    "    tar.extractall(path=cifar_tar.parent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3196d517-ddc1-47cc-94b5-e0bf1ee40481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Convert CIFAR batches into PNG/JPG files and keep only non-cat classes\n",
    "#    CIFAR-10 python batches store images in arrays, labels in a list.\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "cifar_base = Path(\"data/raw/cifar10/cifar-10-batches-py\")\n",
    "out_dir = Path(\"data/processed/noncats/images\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def unpickle(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f, encoding=\"bytes\")\n",
    "\n",
    "# Load label names\n",
    "meta = unpickle(cifar_base / \"batches.meta\")\n",
    "label_names = [x.decode(\"utf-8\") for x in meta[b\"label_names\"]]\n",
    "\n",
    "cat_label_index = label_names.index(\"cat\")  # exclude this from negatives\n",
    "\n",
    "batch_files = [cifar_base / f\"data_batch_{i}\" for i in range(1, 6)]\n",
    "img_count = 0\n",
    "\n",
    "for bf in batch_files:\n",
    "    batch = unpickle(bf)\n",
    "    data = batch[b\"data\"]          # shape: (10000, 3072)\n",
    "    labels = batch[b\"labels\"]      # list of ints\n",
    "\n",
    "    for i, y in enumerate(labels):\n",
    "        if y == cat_label_index:\n",
    "            continue  # skip cat class\n",
    "\n",
    "        # Convert 3072 vector to (32,32,3)\n",
    "        img = data[i].reshape(3, 32, 32).transpose(1, 2, 0)\n",
    "        Image.fromarray(img).save(out_dir / f\"cifar_{img_count:06d}.png\")\n",
    "        img_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6099ff85-551c-4839-b699-f1779fd9b12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded:\n",
      "  s3://sagemaker-us-east-1-549206572067/cat-landmarks-project/raw/cats.zip\n",
      "  s3://sagemaker-us-east-1-549206572067/cat-landmarks-project/raw/cifar_noncats_15000.zip\n"
     ]
    }
   ],
   "source": [
    "# Create zip archives for faster S3 upload \n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import random\n",
    "import boto3\n",
    "\n",
    "bucket = \"sagemaker-us-east-1-549206572067\"\n",
    "prefix = \"cat-landmarks-project\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def zip_folder(src_dir: str, zip_path: str, glob_pattern: str = \"*\") -> None:\n",
    "    src = Path(src_dir)\n",
    "    out = Path(zip_path)\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(out, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        for p in src.rglob(glob_pattern):\n",
    "            if p.is_file():\n",
    "                z.write(p, arcname=p.relative_to(src).as_posix())\n",
    "\n",
    "def zip_sample(files, src_root: Path, zip_path: str, seed: int = 42) -> None:\n",
    "    # Zips only a sampled subset of files (for CIFAR non-cats)\n",
    "    random.seed(seed)\n",
    "    out = Path(zip_path)\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    sampled = random.sample(files, k=min(15000, len(files)))\n",
    "\n",
    "    with zipfile.ZipFile(out, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        for p in sampled:\n",
    "            z.write(p, arcname=p.relative_to(src_root).as_posix())\n",
    "\n",
    "def upload_to_s3(local_path: str, s3_key: str) -> None:\n",
    "    # Uploads a single large file to S3 (fast)\n",
    "    s3.upload_file(local_path, bucket, s3_key)\n",
    "\n",
    "# 1) Zip ALL Kaggle cat images (adjust folder to where your cat .jpg files actually are)\n",
    "cats_images_dir = \"data/raw/cats\"  # change if your unzip created a subfolder\n",
    "cats_zip = \"data/zips/cats.zip\"\n",
    "zip_folder(cats_images_dir, cats_zip, glob_pattern=\"*.jpg\")\n",
    "\n",
    "# 2) Zip ONLY 15,000 CIFAR non-cat images (from the folder you saved them into)\n",
    "noncats_dir = Path(\"data/processed/noncats/images\")  # where you saved CIFAR non-cats\n",
    "noncat_files = [p for p in noncats_dir.glob(\"*.png\") if p.is_file()]\n",
    "cifar_zip = \"data/zips/cifar_noncats_15000.zip\"\n",
    "zip_sample(noncat_files, noncats_dir, cifar_zip)\n",
    "\n",
    "# 3) Upload zip files\n",
    "upload_to_s3(cats_zip,  f\"{prefix}/raw/cats.zip\")\n",
    "upload_to_s3(cifar_zip, f\"{prefix}/raw/cifar_noncats_15000.zip\")\n",
    "\n",
    "print(\"Uploaded:\")\n",
    "print(f\"  s3://{bucket}/{prefix}/raw/cats.zip\")\n",
    "print(f\"  s3://{bucket}/{prefix}/raw/cifar_noncats_15000.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f2030-a79a-4e8a-92c6-1342754ce361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "297db15b-aeab-4774-aea2-ee4db7316bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projectwa\n",
      "sagemaker-studio-549206572067-xrklu309ilg\n",
      "sagemaker-studio-549206572067-zyour2f075q\n",
      "sagemaker-us-east-1-549206572067\n"
     ]
    }
   ],
   "source": [
    "# List buckets you can access\n",
    "import boto3\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "resp = s3.list_buckets()\n",
    "for b in resp.get(\"Buckets\", []):\n",
    "    print(b[\"Name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cefe6bc4-c35e-4bc8-a487-95ce04f5d039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPG files inside cats.zip: 9997\n",
      "Example entries: ['CAT_00/00000001_000.jpg', 'CAT_00/00000001_005.jpg', 'CAT_00/00000001_008.jpg', 'CAT_00/00000001_011.jpg', 'CAT_00/00000001_012.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Check if cats.zip actually contains images\n",
    "# - If count is 0 or very small, the source folder was wrong\n",
    "\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "cats_zip = Path(\"data/zips/cats.zip\")\n",
    "with zipfile.ZipFile(cats_zip, \"r\") as z:\n",
    "    jpgs = [n for n in z.namelist() if n.lower().endswith(\".jpg\")]\n",
    "print(\"JPG files inside cats.zip:\", len(jpgs))\n",
    "print(\"Example entries:\", jpgs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc1ba6d-747b-4559-90ac-8faed8bf435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped to: data/unzipped_from_s3\n",
      "Top folders: [PosixPath('data/unzipped_from_s3/cats'), PosixPath('data/unzipped_from_s3/cifar_noncats_15000')]\n"
     ]
    }
   ],
   "source": [
    "# Unzip raw zips locally so you can upload images to processed/\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "local_out = Path(\"data/unzipped_from_s3\")\n",
    "local_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for zip_name in [\"cats.zip\", \"cifar_noncats_15000.zip\"]:\n",
    "    zp = Path(\"data/zips\") / zip_name\n",
    "    with zipfile.ZipFile(zp, \"r\") as z:\n",
    "        z.extractall(local_out / zip_name.replace(\".zip\", \"\"))\n",
    "\n",
    "print(\"Unzipped to:\", local_out)\n",
    "print(\"Top folders:\", list(local_out.iterdir()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da603dc3-f017-4bbd-ae95-b04c73d8b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded cats: 9997\n",
      "Uploaded noncats: 15000\n"
     ]
    }
   ],
   "source": [
    "# Uploading unzipped images into raw/\n",
    "# - Cats: all jpg\n",
    "# - Noncats: png\n",
    "\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket = \"sagemaker-us-east-1-549206572067\"\n",
    "base_prefix = \"cat-landmarks-project/raw\"\n",
    "\n",
    "def upload_images(src_dir: Path, s3_prefix: str, exts: tuple[str, ...]) -> int:\n",
    "    count = 0\n",
    "    src_dir = Path(src_dir)\n",
    "\n",
    "    for p in src_dir.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in exts:\n",
    "            rel = p.relative_to(src_dir).as_posix()\n",
    "            key = f\"{base_prefix}/{s3_prefix}/{rel}\"\n",
    "            s3.upload_file(str(p), bucket, key)\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "cats_count = upload_images(local_out / \"cats\", \"cats/images\", (\".jpg\",)) \n",
    "noncats_count = upload_images(local_out / \"cifar_noncats_15000\", \"noncats/images\", (\".png\",)) \n",
    "\n",
    "print(\"Uploaded cats:\", cats_count) \n",
    "print(\"Uploaded noncats:\", noncats_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b064e6-ed4a-462a-9680-3100011bb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combining cat and non cat with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3621ec5-e5c6-4171-aeda-aa9510fd5519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    15000\n",
      "1     9997\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_248/49222822.py:37: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  now = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s3_uri</th>\n",
       "      <th>label</th>\n",
       "      <th>ingest_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              s3_uri  label  \\\n",
       "0  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "1  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "2  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "3  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "4  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "\n",
       "                  ingest_time  \n",
       "0  2026-01-30T07:16:06.552664  \n",
       "1  2026-01-30T07:16:06.552664  \n",
       "2  2026-01-30T07:16:06.552664  \n",
       "3  2026-01-30T07:16:06.552664  \n",
       "4  2026-01-30T07:16:06.552664  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a combined manifest from S3 raw images (cats + noncats)\n",
    "# - Writes a single \"processed manifest\" you can use for Athena/EDA/training\n",
    "# - Does not copy images (faster, cleaner, no extra storage)\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "bucket = \"sagemaker-us-east-1-549206572067\"\n",
    "cats_prefix = \"cat-landmarks-project/raw/cats/images/\"\n",
    "noncats_prefix = \"cat-landmarks-project/raw/noncats/images/\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def list_all_keys(prefix: str) -> list[str]:\n",
    "    keys = []\n",
    "    token = None\n",
    "    while True:\n",
    "        kwargs = {\"Bucket\": bucket, \"Prefix\": prefix, \"MaxKeys\": 1000}\n",
    "        if token:\n",
    "            kwargs[\"ContinuationToken\"] = token\n",
    "        resp = s3.list_objects_v2(**kwargs)\n",
    "        for obj in resp.get(\"Contents\", []):\n",
    "            k = obj[\"Key\"]\n",
    "            # Keep only images\n",
    "            if k.lower().endswith((\".jpg\", \".png\")):\n",
    "                keys.append(k)\n",
    "        if not resp.get(\"IsTruncated\"):\n",
    "            break\n",
    "        token = resp.get(\"NextContinuationToken\")\n",
    "    return keys\n",
    "\n",
    "cat_keys = list_all_keys(cats_prefix)\n",
    "noncat_keys = list_all_keys(noncats_prefix)\n",
    "\n",
    "rows = []\n",
    "now = datetime.utcnow().isoformat()\n",
    "\n",
    "for k in cat_keys:\n",
    "    rows.append({\"s3_uri\": f\"s3://{bucket}/{k}\", \"label\": 1, \"ingest_time\": now})\n",
    "\n",
    "for k in noncat_keys:\n",
    "    rows.append({\"s3_uri\": f\"s3://{bucket}/{k}\", \"label\": 0, \"ingest_time\": now})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df[\"label\"].value_counts())\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "368b05a4-5490-46a2-a1fc-41b8858b31ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Combined cat and non cat files  saved to:\n",
      "s3://sagemaker-us-east-1-549206572067/cat-landmarks-project/processed/combined/image_combined.parquet\n"
     ]
    }
   ],
   "source": [
    "# Save processed manifest to S3 (Parquet)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "bucket = \"sagemaker-us-east-1-549206572067\"\n",
    "\n",
    "def get_image_metadata(s3_uri: str) -> tuple[int | None, int | None, int | None]:\n",
    "    # Parse bucket/key from s3:// URI\n",
    "    parts = s3_uri.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "    bucket_name, key = parts[0], parts[1]\n",
    "\n",
    "    try:\n",
    "        # Get file size without downloading the object\n",
    "        head = s3.head_object(Bucket=bucket_name, Key=key)\n",
    "        file_size = head[\"ContentLength\"]\n",
    "\n",
    "        # Download image only to read dimensions\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        with Image.open(BytesIO(obj[\"Body\"].read())) as im:\n",
    "            width, height = im.size\n",
    "\n",
    "        return file_size, width, height\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "\n",
    "# Apply metadata extraction\n",
    "meta = df[\"s3_uri\"].apply(get_image_metadata)\n",
    "df[\"file_size\"] = meta.apply(lambda x: x[0])\n",
    "df[\"width\"] = meta.apply(lambda x: x[1])\n",
    "df[\"height\"] = meta.apply(lambda x: x[2])\n",
    "\n",
    "\n",
    "out_dir = Path(\"data/processed\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "combined_path = out_dir / \"image_combined.parquet\"\n",
    "df.to_parquet(combined_path, index=False)\n",
    "\n",
    "processed_manifest_s3_prefix = \"cat-landmarks-project/processed/combined/\"\n",
    "s3.upload_file(\n",
    "    str(combined_path),\n",
    "    bucket,\n",
    "    f\"{processed_manifest_s3_prefix}image_combined.parquet\"\n",
    ")\n",
    "print(\" Combined cat and non cat files  saved to:\")\n",
    "print(f\"s3://{bucket}/{processed_manifest_s3_prefix}image_combined.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a0d76d-e7e6-4a24-85d4-f2767248dda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s3_uri</th>\n",
       "      <th>label</th>\n",
       "      <th>ingest_time</th>\n",
       "      <th>file_size</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "      <td>469801</td>\n",
       "      <td>375</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "      <td>469801</td>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "      <td>469801</td>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "      <td>310372</td>\n",
       "      <td>500</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://sagemaker-us-east-1-549206572067/cat-land...</td>\n",
       "      <td>1</td>\n",
       "      <td>2026-01-30T07:16:06.552664</td>\n",
       "      <td>310372</td>\n",
       "      <td>500</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              s3_uri  label  \\\n",
       "0  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "1  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "2  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "3  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "4  s3://sagemaker-us-east-1-549206572067/cat-land...      1   \n",
       "\n",
       "                  ingest_time  file_size  width  height  \n",
       "0  2026-01-30T07:16:06.552664     469801    375     500  \n",
       "1  2026-01-30T07:16:06.552664     469801    500     375  \n",
       "2  2026-01-30T07:16:06.552664     469801    500     375  \n",
       "3  2026-01-30T07:16:06.552664     310372    500     375  \n",
       "4  2026-01-30T07:16:06.552664     310372    500     333  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "674a921b-f8ec-475c-a177-4b73763ee9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make landmarks.s3_uri match image_combined.s3_uri (full S3 URI)\n",
    "# 1) Rename image -> s3_uri (if needed)\n",
    "# 2) Convert 'data/raw/...' into 's3://bucket/cat-landmarks-project/raw/...'\n",
    "# 3) Write parquet and upload to the annotations prefix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "bucket = \"sagemaker-us-east-1-549206572067\"\n",
    "\n",
    "ann = pd.read_csv(\"data/processed/cats/annotations.csv\")\n",
    "\n",
    "# If your CSV column is still called \"image\", map it\n",
    "if \"image\" in ann.columns and \"s3_uri\" not in ann.columns:\n",
    "    ann = ann.rename(columns={\"image\": \"s3_uri\"})\n",
    "\n",
    "# Convert to full S3 URI so it matches image_combined.s3_uri exactly\n",
    "ann[\"s3_uri\"] = ann[\"s3_uri\"].str.replace(\n",
    "    r\"^data/raw/\",\n",
    "    f\"s3://{bucket}/cat-landmarks-project/raw/cats/images/\",\n",
    "    regex=True\n",
    ")\n",
    "\n",
    "# Rename landmark columns (same mapping you already used)\n",
    "rename_map = {\n",
    "    \"x1\": \"left_eye_x\", \"y1\": \"left_eye_y\",\n",
    "    \"x2\": \"right_eye_x\",\"y2\": \"right_eye_y\",\n",
    "    \"x3\": \"mouth_x\",    \"y3\": \"mouth_y\",\n",
    "    \"x4\": \"left_ear_1_x\",\"y4\": \"left_ear_1_y\",\n",
    "    \"x5\": \"left_ear_2_x\",\"y5\": \"left_ear_2_y\",\n",
    "    \"x6\": \"left_ear_3_x\",\"y6\": \"left_ear_3_y\",\n",
    "    \"x7\": \"right_ear_1_x\",\"y7\": \"right_ear_1_y\",\n",
    "    \"x8\": \"right_ear_2_x\",\"y8\": \"right_ear_2_y\",\n",
    "    \"x9\": \"right_ear_3_x\",\"y9\": \"right_ear_3_y\",\n",
    "}\n",
    "ann = ann.rename(columns=rename_map)\n",
    "\n",
    "# Force numeric for Athena friendliness\n",
    "landmark_cols = [c for c in ann.columns if c != \"s3_uri\"]\n",
    "ann[landmark_cols] = ann[landmark_cols].apply(pd.to_numeric, errors=\"coerce\").astype(\"float64\")\n",
    "\n",
    "# Write parquet\n",
    "local_parquet = \"data/processed/cats/landmarks.parquet\"\n",
    "ann.to_parquet(local_parquet, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "472b04f5-6680-4e81-86d4-beb369b25d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the annotations Parquet file to S3 for Athena\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "s3.upload_file(\n",
    "    \"data/processed/cats/landmarks.parquet\",\n",
    "    \"sagemaker-us-east-1-549206572067\",\n",
    "    \"cat-landmarks-project/processed/combined/annotations/landmarks.parquet\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4ceef8c-b4dc-421a-be4e-88c5a1b58d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied to: s3://sagemaker-us-east-1-549206572067/cat-landmarks-project/processed/combined/metadata/image_combined.parquet\n"
     ]
    }
   ],
   "source": [
    "# Copy image_combined.parquet into a dedicated metadata/ folder (recommended layout)\n",
    "\n",
    "import boto3\n",
    "\n",
    "bucket = \"sagemaker-us-east-1-549206572067\"\n",
    "\n",
    "src_key = \"cat-landmarks-project/processed/combined/image_combined.parquet\"\n",
    "dst_key = \"cat-landmarks-project/processed/combined/metadata/image_combined.parquet\"\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n",
    "\n",
    "# Copy\n",
    "s3.copy_object(\n",
    "    Bucket=bucket,\n",
    "    CopySource={\"Bucket\": bucket, \"Key\": src_key},\n",
    "    Key=dst_key\n",
    ")\n",
    "\n",
    "# Optional: delete original after confirming copy worked\n",
    "# s3.delete_object(Bucket=bucket, Key=src_key)\n",
    "\n",
    "print(\"Copied to:\", f\"s3://{bucket}/{dst_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61447c3d-bc5d-47d3-91fe-214ddeadb648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_bucket' (str)\n",
      "Stored 'project_prefix' (str)\n",
      "Stored 's3_raw_cats_prefix' (str)\n",
      "Stored 's3_raw_noncats_prefix' (str)\n",
      "Stored 's3_processed_combined_prefix' (str)\n",
      "Stored 'ingestion_completed' (bool)\n"
     ]
    }
   ],
   "source": [
    "# Project-level S3 locations (store once)\n",
    "\n",
    "s3_bucket = \"sagemaker-us-east-1-549206572067\"\n",
    "project_prefix = \"cat-landmarks-project\"\n",
    "\n",
    "s3_raw_cats_prefix = f\"s3://{s3_bucket}/{project_prefix}/raw/cats/images/\"\n",
    "s3_raw_noncats_prefix = f\"s3://{s3_bucket}/{project_prefix}/raw/noncats/images/\"\n",
    "s3_processed_combined_prefix = f\"s3://{s3_bucket}/{project_prefix}/processed/combined/\"\n",
    "\n",
    "ingestion_completed = True\n",
    "\n",
    "%store s3_bucket\n",
    "%store project_prefix\n",
    "%store s3_raw_cats_prefix\n",
    "%store s3_raw_noncats_prefix\n",
    "%store s3_processed_combined_prefix\n",
    "%store ingestion_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b02f0b-89d1-48b3-bc8c-7c62f50e39e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
